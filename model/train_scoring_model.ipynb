{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posture Scoring Model Training\n",
    "\n",
    "This notebook trains an LSTM regression model to predict posture quality scores (0-100) from skeletal keypoint sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.posture_dataset import PostureDataset\n",
    "from model.feature_extractor import NUM_FEATURES, get_feature_names\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"data/neck_stretch\"\n",
    "SEQUENCE_LENGTH = 30  # 30 frames per sequence (~1 second at 30fps)\n",
    "BATCH_SIZE = 16\n",
    "OVERLAP = 0.5  # 50% overlap between windows\n",
    "\n",
    "# Load dataset\n",
    "dataset = PostureDataset(\n",
    "    data_dir=DATA_DIR,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    overlap=OVERLAP,\n",
    ")\n",
    "\n",
    "# Get all data for train/test split\n",
    "X, y = dataset.get_all_data()\n",
    "print(f\"\\nDataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Number of features: {NUM_FEATURES}\")\n",
    "print(f\"Feature names: {get_feature_names()[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80/20 split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"\\nScore range: {y_train.min() * 100:.1f} - {y_train.max() * 100:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.scoring_model import (\n",
    "    create_scoring_model,\n",
    "    compile_model,\n",
    "    create_callbacks,\n",
    "    get_model_summary,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = create_scoring_model(\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    num_features=NUM_FEATURES,\n",
    "    lstm_units=(64, 32),\n",
    "    dropout_rate=0.3,\n",
    "    dense_units=16,\n",
    "    use_gru=False,  # Use LSTM\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model = compile_model(\n",
    "    model,\n",
    "    learning_rate=0.001,\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(get_model_summary(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "SAVE_DIR = \"save\"\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = create_callbacks(\n",
    "    save_dir=SAVE_DIR,\n",
    "    patience=15,\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "axes[0].plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"MSE Loss\")\n",
    "axes[0].set_title(\"Training & Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAE plot\n",
    "axes[1].plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "axes[1].plot(history.history[\"val_mae\"], label=\"Val MAE\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"MAE\")\n",
    "axes[1].set_title(\"Training & Validation MAE\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"training_history.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Validation MSE Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Validation MAE (0-100 scale): {val_mae * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(SAVE_DIR, \"posture_scorer.h5\")\n",
    "model.save(final_model_path)\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "\n",
    "# Also save as SavedModel format for better compatibility\n",
    "savedmodel_path = os.path.join(SAVE_DIR, \"posture_scorer_savedmodel\")\n",
    "model.save(savedmodel_path)\n",
    "print(f\"SavedModel saved to: {savedmodel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference test\n",
    "loaded_model = tf.keras.models.load_model(final_model_path)\n",
    "\n",
    "# Make predictions on validation set\n",
    "predictions = loaded_model.predict(X_val[:5], verbose=0)\n",
    "\n",
    "print(\"\\nSample Predictions vs Ground Truth:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (pred, true) in enumerate(zip(predictions, y_val[:5])):\n",
    "    print(f\"Sample {i + 1}: Predicted={pred[0] * 100:.1f}, True={true * 100:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
